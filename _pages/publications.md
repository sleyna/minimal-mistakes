---
layout: single
title: "Publications"
permalink: /publications/
---

Below is a list of my publications.

<table style="width: 100%; table-layout: auto; border-collapse: collapse;">
  <thead>
    <tr>
      <th style="text-align: left; white-space: nowrap;">Date</th>
      <th style="text-align: left;">Title</th>
      <th style="text-align: left;">Authors</th>
      <th style="text-align: left; white-space: nowrap;">Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="white-space: nowrap;">2025-03-28</td>
      <td>Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)</td>
      <td><strong>Lena Strobl</strong>, Dana Angluin, Robert Frank</td>
      <td><a href="https://arxiv.org/abs/2503.22076">arXiv</a></td>
    </tr>
    <tr>
      <td style="white-space: nowrap;">2024-12-13</td>
      <td>Simulating Hard Attention Using Soft Attention</td>
      <td>Andy J Yang, <strong>Lena Strobl</strong>, David Chiang, Dana Angluin</td>
      <td><a href="https://arxiv.org/abs/2412.09925">arXiv</a></td>
    </tr>
    <tr>
      <td style="white-space: nowrap;">2024-04-02</td>
      <td>Transformers as Transducers</td>
      <td><strong>Lena Strobl</strong>, Dana Angluin, David Chiang, Jon Rawski, Ashish Sabharwal</td>
      <td><a href="https://arxiv.org/abs/2404.02040">arXiv</a></td>
    </tr>
    <tr>
      <td style="white-space: nowrap;">2023-11-01</td>
      <td>What Formal Languages Can Transformers Express? A Survey</td>
      <td><strong>Lena Strobl</strong>, Will Merrill, Gail Weiss, David Chiang, Dana Angluin</td>
      <td><a href="https://arxiv.org/abs/2311.00208">arXiv</a></td>
    </tr>
    <tr>
      <td style="white-space: nowrap;">2023-08-06</td>
      <td>Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits</td>
      <td><strong>Lena Strobl</strong></td>
      <td><a href="https://arxiv.org/abs/2308.03212">arXiv</a></td>
    </tr>
  </tbody>
</table>
